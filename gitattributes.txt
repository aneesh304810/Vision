
"""
========================================================
Enterprise Data Explorer 360¬∞ - Enhanced Semantic Matching
Optimized for your specific data structure
========================================================
"""

import streamlit as st
import pandas as pd
import json
import re
import os
from collections import defaultdict
import numpy as np

# ML/NLP
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
    from sentence_transformers import SentenceTransformer, util
    ML_AVAILABLE = True
except:
    ML_AVAILABLE = False
    st.warning("‚ö†Ô∏è ML libraries not available. Semantic matching will be limited.")

# SQL Parsing
try:
    import sqlglot
    from sqlglot.expressions import Table, Column, Join
    SQLGLOT_AVAILABLE = True
except:
    SQLGLOT_AVAILABLE = False

# Visualization  
try:
    from streamlit_agraph import agraph, Node, Edge, Config
    AGRAPH_AVAILABLE = True
except:
    AGRAPH_AVAILABLE = False

try:
    from st_aggrid import AgGrid, GridOptionsBuilder, GridUpdateMode
    AGGRID_AVAILABLE = True
except:
    AGGRID_AVAILABLE = False

# Configuration
st.set_page_config(
    page_title="Data Explorer 360¬∞ Pro",
    page_icon="üåê",
    layout="wide",
    initial_sidebar_state="expanded"
)

if ML_AVAILABLE:
    torch.set_num_threads(8)

MAPPING_FILE = "interface_sql_mapping.xlsx"

# ========================================================
# PREMIUM STYLING
# ========================================================

st.markdown("""
<style>
    .premium-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 50%, #f093fb 100%);
        padding: 2rem;
        border-radius: 15px;
        text-align: center;
        margin-bottom: 2rem;
        box-shadow: 0 10px 30px rgba(102, 126, 234, 0.3);
    }
    
    .premium-title {
        font-size: 3.5rem;
        font-weight: 800;
        color: white;
        margin: 0;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
    }
    
    .breadcrumb {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem 2rem;
        border-radius: 12px;
        margin-bottom: 2rem;
        color: white;
        font-size: 1.1rem;
    }
    
    .metric-card {
        background: white;
        padding: 2rem;
        border-radius: 15px;
        box-shadow: 0 5px 20px rgba(0,0,0,0.08);
        text-align: center;
        transition: all 0.3s ease;
        border-top: 4px solid #667eea;
    }
    
    .metric-card:hover {
        transform: translateY(-8px);
        box-shadow: 0 10px 30px rgba(102, 126, 234, 0.2);
    }
    
    .metric-value {
        font-size: 3rem;
        font-weight: 800;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    
    .interactive-card {
        background: white;
        padding: 1.5rem;
        border-radius: 12px;
        border-left: 5px solid #667eea;
        margin: 1rem 0;
        box-shadow: 0 3px 15px rgba(0,0,0,0.08);
        cursor: pointer;
        transition: all 0.3s ease;
    }
    
    .interactive-card:hover {
        transform: translateX(10px);
        box-shadow: 0 5px 25px rgba(102, 126, 234, 0.15);
    }
    
    .ai-insight {
        background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
        border-radius: 12px;
        padding: 2rem;
        margin: 2rem 0;
        border-left: 6px solid #ff6b6b;
    }
    
    .confidence-high {
        background: #e8f5e9;
        color: #2e7d32;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-weight: 700;
    }
    
    .confidence-medium {
        background: #fff3e0;
        color: #ef6c00;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-weight: 700;
    }
    
    .confidence-low {
        background: #fce4ec;
        color: #c2185b;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-weight: 700;
    }
    
    .column-badge {
        display: inline-block;
        background: #e3f2fd;
        color: #1976d2;
        padding: 0.4rem 1rem;
        border-radius: 15px;
        margin: 0.3rem;
        font-size: 0.9rem;
    }
    
    .active-filter-panel {
        background: #fff3e0;
        border-left: 4px solid #ff9800;
        padding: 1rem;
        border-radius: 8px;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# ========================================================
# SESSION STATE
# ========================================================

if 'navigation_level' not in st.session_state:
    st.session_state.navigation_level = 'overview'
if 'selected_source' not in st.session_state:
    st.session_state.selected_source = None
if 'selected_target' not in st.session_state:
    st.session_state.selected_target = None  
if 'selected_interface' not in st.session_state:
    st.session_state.selected_interface = None
if 'selected_sql' not in st.session_state:
    st.session_state.selected_sql = None

# Data
if 'interface_df' not in st.session_state:
    st.session_state.interface_df = None
if 'sql_df' not in st.session_state:
    st.session_state.sql_df = None
if 'mapping_df' not in st.session_state:
    st.session_state.mapping_df = None

# GLOBAL FILTERS
if 'filter_source_systems' not in st.session_state:
    st.session_state.filter_source_systems = []
if 'filter_target_systems' not in st.session_state:
    st.session_state.filter_target_systems = []
if 'filter_types' not in st.session_state:
    st.session_state.filter_types = []
if 'filter_applications' not in st.session_state:
    st.session_state.filter_applications = []

# ========================================================
# LOAD MODELS
# ========================================================

@st.cache_resource(show_spinner=False)
def load_models():
    """Load models optimized for CPU"""
    embedding = None
    llm = None
    
    if ML_AVAILABLE:
        try:
            st.info("üîÑ Loading embedding model (lightweight, CPU-friendly)...")
            embedding = SentenceTransformer("all-MiniLM-L6-v2")
            st.success("‚úÖ Embedding model loaded - Semantic matching enabled!")
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Could not load embedding model: {e}")
            st.info("üí° Install with: pip install sentence-transformers")
    else:
        st.warning("‚ö†Ô∏è ML libraries not installed. Using deterministic matching only.")
        st.info("üí° For better matching, install: pip install torch sentence-transformers")
    
    # Note: We don't load Phi-3 LLM as it requires 4GB+ RAM and GPU
    # Instead, we use intelligent rule-based explanations
    st.info("‚ÑπÔ∏è Using rule-based explanations (fast, CPU-friendly, no GPU needed)")
    
    return embedding, llm

embedding_model, llm_model = load_models()

# ========================================================
# RULE-BASED EXPLANATION GENERATOR (CPU-FRIENDLY)
# ========================================================

def generate_business_explanation_rules(interface_row, sql_row, tables_list):
    """Generate business explanation using rules (no LLM needed)"""
    
    application = safe_get(interface_row, 'application', 'the system')
    integration = safe_get(interface_row, 'integration', 'this interface')
    description = safe_get(interface_row, 'description', '')
    source = safe_get(interface_row, 'source_system', 'source')
    target = safe_get(interface_row, 'target_system', 'target')
    queryname = safe_get(sql_row, 'queryname', 'this query')
    
    # Start with purpose
    explanation = f"**Business Purpose:**\n\n"
    
    if description:
        explanation += f"{description}\n\n"
    else:
        explanation += f"This interface ({integration}) transfers data from {source} to {target}.\n\n"
    
    # Data source
    explanation += f"**Data Source:**\n\n"
    explanation += f"The data originates from the {application} application"
    if tables_list:
        explanation += f" and retrieves information from {len(tables_list)} database table(s): "
        explanation += ", ".join(tables_list[:3])
        if len(tables_list) > 3:
            explanation += f", and {len(tables_list) - 3} more"
    explanation += ".\n\n"
    
    # Business value
    explanation += f"**Business Value:**\n\n"
    
    # Detect common patterns
    if any(word in queryname.lower() for word in ['customer', 'client', 'account']):
        explanation += "This supports customer relationship management and account tracking. "
    elif any(word in queryname.lower() for word in ['transaction', 'trade', 'payment']):
        explanation += "This enables transaction processing and financial record-keeping. "
    elif any(word in queryname.lower() for word in ['position', 'holding', 'portfolio']):
        explanation += "This provides portfolio management and position tracking. "
    elif any(word in queryname.lower() for word in ['balance', 'reconciliation', 'settlement']):
        explanation += "This ensures accurate balance tracking and financial reconciliation. "
    else:
        explanation += "This provides critical data integration between systems. "
    
    # Frequency
    if 'daily' in queryname.lower() or 'daily' in description.lower():
        explanation += "The data is refreshed daily to ensure business users have current information."
    elif 'real' in queryname.lower() or 'intraday' in description.lower():
        explanation += "The data is updated in real-time to support immediate business decisions."
    else:
        explanation += "The data is synchronized regularly to maintain accuracy across systems."
    
    return explanation

def generate_technical_explanation_rules(sql_row, tables_list, analysis):
    """Generate technical explanation using rules (no LLM needed)"""
    
    queryname = safe_get(sql_row, 'queryname', 'Query')
    sql_file = safe_get(sql_row, 'file', '')
    system = safe_get(sql_row, 'system', '')
    complexity = analysis.get('complexity', 'Unknown')
    
    explanation = f"**Technical Overview:**\n\n"
    
    # Query identification
    explanation += f"Query Name: `{queryname}`\n"
    if sql_file:
        explanation += f"File Location: `{sql_file}`\n"
    if system:
        explanation += f"Source System: {system}\n"
    explanation += f"Complexity: **{complexity}**\n\n"
    
    # Data structures
    explanation += f"**Data Structures:**\n\n"
    if tables_list:
        explanation += f"This query accesses {len(tables_list)} table(s):\n"
        for i, table in enumerate(tables_list[:5], 1):
            explanation += f"{i}. `{table}`\n"
        if len(tables_list) > 5:
            explanation += f"... and {len(tables_list) - 5} more tables\n"
    
    # Operations
    explanation += f"\n**Operations:**\n\n"
    
    if len(analysis.get('joins', [])) > 0:
        explanation += f"- **{len(analysis['joins'])} table join(s)**: "
        explanation += "Combines data from multiple sources using relational keys.\n"
    
    if analysis.get('has_subquery'):
        explanation += "- **Subqueries**: Uses nested queries for complex data filtering.\n"
    
    if analysis.get('has_group_by'):
        explanation += "- **Aggregation**: Groups and summarizes data for reporting.\n"
    
    if analysis.get('where_conditions'):
        explanation += "- **Filtering**: Applies business rules to select relevant records.\n"
    
    # Performance considerations
    explanation += f"\n**Performance Considerations:**\n\n"
    
    if complexity == 'High':
        explanation += "‚ö†Ô∏è This is a complex query. Ensure proper indexing on join columns. "
        explanation += "Consider reviewing execution plan for optimization opportunities.\n"
    elif complexity == 'Medium':
        explanation += "‚ÑπÔ∏è Moderate complexity. Standard indexing should provide good performance. "
        explanation += "Monitor execution time during peak loads.\n"
    else:
        explanation += "‚úì Simple query structure. Should execute efficiently with basic indexing.\n"
    
    # Recommendations
    if len(tables_list) > 5:
        explanation += "\nüí° **Tip**: With {} tables, verify that all joins are necessary and properly indexed.".format(len(tables_list))
    
    return explanation

# ========================================================
# SQL ANALYSIS (CPU-FRIENDLY)
# ========================================================

def analyze_sql_query(raw_sql):
    """Analyze SQL query structure without heavy parsing"""
    
    analysis = {
        'tables': [],
        'columns': [],
        'joins': [],
        'where_conditions': '',
        'has_subquery': False,
        'has_union': False,
        'has_group_by': False,
        'has_order_by': False,
        'complexity': 'Low'
    }
    
    if not raw_sql or not isinstance(raw_sql, str):
        return analysis
    
    sql_upper = raw_sql.upper()
    
    # Basic pattern extraction (lightweight, no heavy parsing)
    # Extract tables
    from_pattern = re.findall(r'FROM\s+([^\s,;()\n]+)', sql_upper, re.IGNORECASE)
    join_pattern = re.findall(r'JOIN\s+([^\s,;()\n]+)', sql_upper, re.IGNORECASE)
    
    tables = list(set(from_pattern + join_pattern))
    analysis['tables'] = [t.strip() for t in tables if t.strip()]
    
    # Detect features
    analysis['has_subquery'] = '(SELECT' in sql_upper or 'WITH ' in sql_upper[:100]
    analysis['has_union'] = 'UNION' in sql_upper
    analysis['has_group_by'] = 'GROUP BY' in sql_upper
    analysis['has_order_by'] = 'ORDER BY' in sql_upper
    
    # Count joins
    analysis['joins'] = re.findall(r'\bJOIN\b', sql_upper, re.IGNORECASE)
    
    # Extract WHERE clause (simplified)
    where_match = re.search(r'WHERE\s+(.+?)(?:GROUP BY|ORDER BY|LIMIT|$)', sql_upper, re.IGNORECASE | re.DOTALL)
    if where_match:
        analysis['where_conditions'] = where_match.group(1)[:200]  # Limit length
    
    # Complexity scoring
    complexity_score = 0
    complexity_score += len(analysis['tables']) * 2
    complexity_score += len(analysis['joins']) * 3
    complexity_score += 10 if analysis['has_subquery'] else 0
    complexity_score += 5 if analysis['has_group_by'] else 0
    complexity_score += 5 if analysis['has_union'] else 0
    
    if complexity_score < 10:
        analysis['complexity'] = 'Low'
    elif complexity_score < 25:
        analysis['complexity'] = 'Medium'
    else:
        analysis['complexity'] = 'High'
    
    return analysis

def clean_column_name(col):
    """Clean column names"""
    return str(col).strip().lower().replace(" ", "_").replace("/", "_").replace("-", "_").replace("(", "").replace(")", "").strip("_")

def safe_get(row, column, default=""):
    """Safely get value from row"""
    try:
        val = row.get(column, default) if isinstance(row, dict) else getattr(row, column, default)
        return val if pd.notna(val) and str(val) != 'nan' and str(val) != '' else default
    except:
        return default

def normalize_text(text):
    """Normalize text for matching"""
    if not isinstance(text, str):
        return ""
    return text.lower().replace("_", "").replace(" ", "").replace("-", "").strip()

# ========================================================
# DATA LOADING - YOUR SPECIFIC STRUCTURE
# ========================================================

def load_interface_inventory(file):
    """Load interface inventory - YOUR SPECIFIC COLUMNS"""
    
    st.info("üîç Loading Interface Inventory with your column structure...")
    
    # Try different sheet names
    for sheet in ["interface", "Interface", "Sheet1", "Interfaces"]:
        try:
            df = pd.read_excel(file, sheet_name=sheet)
            st.success(f"‚úÖ Found sheet: '{sheet}'")
            break
        except:
            continue
    else:
        df = pd.read_excel(file)
        st.info("‚úÖ Loaded from first sheet")
    
    # Show original columns
    with st.expander("üìã Original Columns Detected"):
        st.write(list(df.columns))
    
    # Clean column names
    df.columns = [clean_column_name(col) for col in df.columns]
    
    # Remove empty columns
    df = df.dropna(axis=1, how='all')
    df = df.loc[:, ~df.columns.str.contains("^unnamed", case=False)]
    
    # Map YOUR specific columns
    column_mappings = {
        'application': ['application', 'app'],
        'integration': ['integration', 'interface'],
        'description': ['description', 'desc'],
        'type': ['type'],
        'source_system': ['source_system', 'source'],
        'target_system': ['target_system', 'target'],
        'inbound_outbound': ['inbound_outbound_with_respect_to_existing_acct_platform', 'inbound_outbound', 'direction'],
        'feed_routing': ['feed_routing', 'routing'],
        'frequency': ['frequency'],
        'owner': ['application_owner_contact', 'owner', 'contact']
    }
    
    for std_name, variations in column_mappings.items():
        for var in variations:
            if var in df.columns and std_name not in df.columns:
                df[std_name] = df[var]
                break
    
    # Ensure minimum required columns
    required = ['application', 'integration', 'source_system', 'target_system']
    missing = [col for col in required if col not in df.columns]
    
    if missing:
        st.error(f"‚ùå Missing required columns: {', '.join(missing)}")
        st.write("Available columns:", list(df.columns))
        return None
    
    # Clean strings
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = df[col].astype(str).str.strip().replace({'nan': '', 'None': '', 'NaN': ''})
    
    # Show mapped columns
    with st.expander("‚úÖ Mapped Columns"):
        st.write({k: v for k, v in column_mappings.items() if k in df.columns})
    
    return df

def load_sql_metadata(file):
    """Load SQL metadata - YOUR SPECIFIC COLUMNS"""
    
    st.info("üîç Loading SQL Metadata with your column structure...")
    
    for sheet in ["Queries", "Sheet1", "SQL", "Output"]:
        try:
            df = pd.read_excel(file, sheet_name=sheet)
            st.success(f"‚úÖ Found sheet: '{sheet}'")
            break
        except:
            continue
    else:
        df = pd.read_excel(file)
        st.info("‚úÖ Loaded from first sheet")
    
    # Show original columns
    with st.expander("üìã SQL Original Columns"):
        st.write(list(df.columns))
    
    # Clean columns
    df.columns = [clean_column_name(col) for col in df.columns]
    
    # Map columns
    mappings = {
        'system': ['system'],
        'file': ['file'],
        'queryname': ['queryname', 'query_name', 'query'],
        'tables': ['tables', 'table']
    }
    
    for std, vars in mappings.items():
        for v in vars:
            if v in df.columns and std not in df.columns:
                df[std] = df[v]
                break
    
    # Check required
    required = ['queryname']
    missing = [col for col in required if col not in df.columns]
    
    if missing:
        st.error(f"‚ùå Missing required SQL columns: {', '.join(missing)}")
        return None
    
    # Clean strings
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = df[col].astype(str).str.strip().replace({'nan': '', 'None': ''})
    
    return df

# ========================================================
# ENHANCED SEMANTIC MATCHING ENGINE
# ========================================================

def calculate_enhanced_match_score(interface_row, sql_row, use_semantic=True):
    """
    OPTIMIZED MATCHING - HIGHEST PRIORITY:
    Interface: Description, Application, Integration
    SQL: File, QueryName
    """
    
    score = 0
    details = []
    
    # Extract key fields
    application = normalize_text(safe_get(interface_row, 'application'))
    integration = normalize_text(safe_get(interface_row, 'integration'))
    description = normalize_text(safe_get(interface_row, 'description'))
    
    queryname = normalize_text(safe_get(sql_row, 'queryname'))
    sql_file = normalize_text(safe_get(sql_row, 'file'))
    
    # === PRIORITY MATCHING: Description/Application/Integration vs File/QueryName ===
    # Total: 80 points for primary matching
    
    # 1. APPLICATION MATCHING (30 points)
    if application:
        app_words = set(application.split())  # Define at the start
        
        # Application in QueryName (20 points)
        if queryname and (application in queryname or queryname in application):
            score += 20
            details.append(f"‚úì Application '{application}' in QueryName")
        elif queryname:
            # Partial word match
            query_words = set(queryname.split())
            common = app_words & query_words
            if common:
                score += min(len(common) * 5, 15)
                details.append(f"‚úì Application words in QueryName ({len(common)})")
        
        # Application in File (10 points)
        if sql_file and (application in sql_file or sql_file in application):
            score += 10
            details.append(f"‚úì Application '{application}' in File")
        elif sql_file:
            # Partial word match
            file_words = set(sql_file.split())
            common = app_words & file_words
            if common:
                score += min(len(common) * 3, 8)
                details.append(f"‚úì Application words in File ({len(common)})")
    
    # 2. INTEGRATION MATCHING (30 points)
    if integration:
        int_words = set(integration.split())  # Define at the start
        
        # Integration in QueryName (20 points)
        if queryname and (integration in queryname or queryname in integration):
            score += 20
            details.append(f"‚úì Integration '{integration}' in QueryName")
        elif queryname:
            # Word-level matching
            query_words = set(queryname.split())
            common = int_words & query_words
            if common:
                score += min(len(common) * 5, 15)
                details.append(f"‚úì Integration words in QueryName ({len(common)})")
        
        # Integration in File (10 points)
        if sql_file and (integration in sql_file or sql_file in integration):
            score += 10
            details.append(f"‚úì Integration '{integration}' in File")
        elif sql_file:
            # Partial word match
            file_words = set(sql_file.split())
            common = int_words & file_words
            if common:
                score += min(len(common) * 3, 8)
                details.append(f"‚úì Integration words in File ({len(common)})")
    
    # 3. DESCRIPTION MATCHING (20 points)
    if description:
        desc_words = set(description.split())
        
        # Description keywords in QueryName (10 points)
        if queryname:
            query_words = set(queryname.split())
            common = desc_words & query_words
            if common:
                score += min(len(common) * 2, 10)
                details.append(f"‚úì Description words in QueryName ({len(common)})")
        
        # Description keywords in File (10 points)
        if sql_file:
            file_words = set(sql_file.split())
            common = desc_words & file_words
            if common:
                score += min(len(common) * 2, 10)
                details.append(f"‚úì Description words in File ({len(common)})")
    
    # === SECONDARY MATCHING: System/Type ===
    # Total: 20 points for supporting evidence
    
    # 4. System Matching (15 points)
    source_sys = normalize_text(safe_get(interface_row, 'source_system'))
    sql_sys = normalize_text(safe_get(sql_row, 'system'))
    
    if source_sys and sql_sys:
        if source_sys == sql_sys:
            score += 15
            details.append(f"‚úì System match: {source_sys}")
        elif source_sys in sql_sys or sql_sys in source_sys:
            score += 10
            details.append(f"‚úì Partial system match")
    
    # 5. Type/Format Hints (5 points)
    int_type = normalize_text(safe_get(interface_row, 'type'))
    if int_type and 'sql' in int_type:
        if (sql_file and 'sql' in sql_file) or (queryname and 'query' in queryname):
            score += 5
            details.append(f"‚úì Type indicator: SQL")
    
    # === SEMANTIC BOOST (FOCUSED ON KEY FIELDS) ===
    # Use ONLY the priority fields for semantic matching
    
    semantic_score = 0
    if use_semantic and embedding_model:
        try:
            # Build focused text - ONLY priority fields
            interface_text = f"{application} {integration} {description}".strip()
            sql_text = f"{queryname} {sql_file}".strip()
            
            if interface_text and sql_text:
                # Calculate semantic similarity
                int_emb = embedding_model.encode(interface_text, convert_to_tensor=True)
                sql_emb = embedding_model.encode(sql_text, convert_to_tensor=True)
                
                similarity = util.cos_sim(int_emb, sql_emb).item()
                
                # Semantic score (0-50 points for very high similarity)
                if similarity >= 0.85:
                    semantic_score = 50
                    details.append(f"ü§ñ Excellent semantic match ({similarity:.2f})")
                elif similarity >= 0.75:
                    semantic_score = 40
                    details.append(f"ü§ñ Very high semantic match ({similarity:.2f})")
                elif similarity >= 0.65:
                    semantic_score = 30
                    details.append(f"ü§ñ High semantic match ({similarity:.2f})")
                elif similarity >= 0.55:
                    semantic_score = 20
                    details.append(f"ü§ñ Good semantic match ({similarity:.2f})")
                elif similarity >= 0.45:
                    semantic_score = 10
                    details.append(f"ü§ñ Moderate semantic match ({similarity:.2f})")
        except Exception as e:
            pass  # Silent fail for semantic errors
    
    final_score = min(score + semantic_score, 100)
    
    return final_score, details, semantic_score

def generate_enhanced_mapping(interface_df, sql_df, min_score=45, use_semantic=True):
    """Generate enhanced semantic mapping with optimized batch processing"""
    
    results = []
    
    st.write(f"üîÑ Starting mapping generation...")
    st.write(f"   Interfaces: {len(interface_df)}")
    st.write(f"   SQL Queries: {len(sql_df)}")
    st.write(f"   Semantic Matching: {'‚úÖ Enabled' if use_semantic and embedding_model else '‚ùå Disabled'}")
    st.write(f"   Minimum Score: {min_score}")
    
    # ========================================================
    # OPTIMIZATION: Pre-compute all embeddings in batch
    # ========================================================
    
    interface_embeddings = None
    sql_embeddings = None
    
    if use_semantic and embedding_model:
        st.write("üöÄ Optimizing: Pre-computing embeddings in batches...")
        
        try:
            # Build interface texts (batch)
            interface_texts = []
            for _, row in interface_df.iterrows():
                application = safe_get(row, 'application', '')
                integration = safe_get(row, 'integration', '')
                description = safe_get(row, 'description', '')
                text = f"{application} {integration} {description}".strip()
                interface_texts.append(text if text else "unknown")
            
            # Build SQL texts (batch)
            sql_texts = []
            for _, row in sql_df.iterrows():
                queryname = safe_get(row, 'queryname', '')
                sql_file = safe_get(row, 'file', '')
                text = f"{queryname} {sql_file}".strip()
                sql_texts.append(text if text else "unknown")
            
            # Compute ALL embeddings at once (MUCH faster)
            st.write(f"   Computing {len(interface_texts)} interface embeddings...")
            interface_embeddings = embedding_model.encode(
                interface_texts, 
                batch_size=32,
                show_progress_bar=False,
                convert_to_numpy=True  # Use numpy for faster comparison
            )
            
            st.write(f"   Computing {len(sql_texts)} SQL embeddings...")
            sql_embeddings = embedding_model.encode(
                sql_texts,
                batch_size=32, 
                show_progress_bar=False,
                convert_to_numpy=True
            )
            
            st.success("‚úÖ Embeddings pre-computed! Matching will be MUCH faster now.")
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Could not pre-compute embeddings: {e}")
            st.info("Falling back to deterministic matching only")
            use_semantic = False
    
    # ========================================================
    # MATCHING WITH PRE-COMPUTED EMBEDDINGS
    # ========================================================
    
    progress = st.progress(0)
    status = st.empty()
    
    total = len(interface_df)
    matched_count = 0
    
    for idx, (i_idx, i_row) in enumerate(interface_df.iterrows()):
        progress.progress((idx + 1) / total)
        
        if idx % 50 == 0:  # Update status every 50 interfaces
            status.text(f"Processing interface {idx + 1} of {total}... (Found {matched_count} matches)")
        
        for s_idx, s_row in sql_df.iterrows():
            # Calculate deterministic score (fast)
            final_score, match_details, _ = calculate_enhanced_match_score_fast(
                i_row, s_row, use_semantic=False
            )
            
            # Add semantic score if enabled (now MUCH faster)
            semantic_score = 0
            if use_semantic and interface_embeddings is not None:
                try:
                    # Use pre-computed embeddings (just dot product, super fast!)
                    similarity = np.dot(interface_embeddings[idx], sql_embeddings[s_idx])
                    
                    # Score based on similarity
                    if similarity >= 0.85:
                        semantic_score = 50
                        match_details.append(f"ü§ñ Excellent semantic match ({similarity:.2f})")
                    elif similarity >= 0.75:
                        semantic_score = 40
                        match_details.append(f"ü§ñ Very high semantic match ({similarity:.2f})")
                    elif similarity >= 0.65:
                        semantic_score = 30
                        match_details.append(f"ü§ñ High semantic match ({similarity:.2f})")
                    elif similarity >= 0.55:
                        semantic_score = 20
                        match_details.append(f"ü§ñ Good semantic match ({similarity:.2f})")
                    elif similarity >= 0.45:
                        semantic_score = 10
                        match_details.append(f"ü§ñ Moderate semantic match ({similarity:.2f})")
                except:
                    pass
            
            final_score = min(final_score + semantic_score, 100)
            
            if final_score >= min_score:
                matched_count += 1
                
                # Determine confidence
                if final_score >= 85:
                    confidence = "High"
                elif final_score >= 70:
                    confidence = "Medium"
                else:
                    confidence = "Low"
                
                results.append({
                    'application': safe_get(i_row, 'application'),
                    'integration': safe_get(i_row, 'integration'),
                    'description': safe_get(i_row, 'description'),
                    'type': safe_get(i_row, 'type'),
                    'source_system': safe_get(i_row, 'source_system'),
                    'target_system': safe_get(i_row, 'target_system'),
                    'inbound_outbound': safe_get(i_row, 'inbound_outbound'),
                    'sql_system': safe_get(s_row, 'system'),
                    'sql_file': safe_get(s_row, 'file'),
                    'queryname': safe_get(s_row, 'queryname'),
                    'tables': safe_get(s_row, 'tables'),
                    'deterministic_score': final_score - semantic_score,
                    'semantic_score': semantic_score,
                    'final_score': final_score,
                    'confidence': confidence,
                    'match_details': '; '.join(match_details)
                })
    
    progress.empty()
    status.empty()
    
    st.success(f"‚úÖ Mapping complete! Found {matched_count} matches")
    
    df = pd.DataFrame(results)
    
    # Ensure all required columns
    if df.empty:
        st.warning("‚ö†Ô∏è No matches found. Try lowering the minimum score.")
        df = pd.DataFrame(columns=[
            'application', 'integration', 'description', 'type',
            'source_system', 'target_system', 'inbound_outbound',
            'sql_system', 'sql_file', 'queryname', 'tables',
            'deterministic_score', 'semantic_score', 'final_score',
            'confidence', 'match_details'
        ])
    
    return df

def calculate_enhanced_match_score_fast(interface_row, sql_row, use_semantic=False):
    """
    Fast version - only deterministic matching
    Semantic matching done separately with pre-computed embeddings
    """
    
    score = 0
    details = []
    
    # Extract key fields
    application = normalize_text(safe_get(interface_row, 'application'))
    integration = normalize_text(safe_get(interface_row, 'integration'))
    description = normalize_text(safe_get(interface_row, 'description'))
    
    queryname = normalize_text(safe_get(sql_row, 'queryname'))
    sql_file = normalize_text(safe_get(sql_row, 'file'))
    
    # === PRIMARY MATCHING (80 points) ===
    
    # 1. APPLICATION MATCHING (30 points)
    if application:
        app_words = set(application.split())
        
        if queryname and (application in queryname or queryname in application):
            score += 20
            details.append(f"‚úì Application '{application}' in QueryName")
        elif queryname:
            query_words = set(queryname.split())
            common = app_words & query_words
            if common:
                score += min(len(common) * 5, 15)
                details.append(f"‚úì Application words in QueryName ({len(common)})")
        
        if sql_file and (application in sql_file or sql_file in application):
            score += 10
            details.append(f"‚úì Application '{application}' in File")
        elif sql_file:
            file_words = set(sql_file.split())
            common = app_words & file_words
            if common:
                score += min(len(common) * 3, 8)
                details.append(f"‚úì Application words in File ({len(common)})")
    
    # 2. INTEGRATION MATCHING (30 points)
    if integration:
        int_words = set(integration.split())
        
        if queryname and (integration in queryname or queryname in integration):
            score += 20
            details.append(f"‚úì Integration '{integration}' in QueryName")
        elif queryname:
            query_words = set(queryname.split())
            common = int_words & query_words
            if common:
                score += min(len(common) * 5, 15)
                details.append(f"‚úì Integration words in QueryName ({len(common)})")
        
        if sql_file and (integration in sql_file or sql_file in integration):
            score += 10
            details.append(f"‚úì Integration '{integration}' in File")
        elif sql_file:
            file_words = set(sql_file.split())
            common = int_words & file_words
            if common:
                score += min(len(common) * 3, 8)
                details.append(f"‚úì Integration words in File ({len(common)})")
    
    # 3. DESCRIPTION MATCHING (20 points)
    if description:
        desc_words = set(description.split())
        
        if queryname:
            query_words = set(queryname.split())
            common = desc_words & query_words
            if common:
                score += min(len(common) * 2, 10)
                details.append(f"‚úì Description words in QueryName ({len(common)})")
        
        if sql_file:
            file_words = set(sql_file.split())
            common = desc_words & file_words
            if common:
                score += min(len(common) * 2, 10)
                details.append(f"‚úì Description words in File ({len(common)})")
    
    # === SECONDARY MATCHING (20 points) ===
    
    # 4. System Matching (15 points)
    source_sys = normalize_text(safe_get(interface_row, 'source_system'))
    sql_sys = normalize_text(safe_get(sql_row, 'system'))
    
    if source_sys and sql_sys:
        if source_sys == sql_sys:
            score += 15
            details.append(f"‚úì System match: {source_sys}")
        elif source_sys in sql_sys or sql_sys in source_sys:
            score += 10
            details.append(f"‚úì Partial system match")
    
    # 5. Type/Format Hints (5 points)
    int_type = normalize_text(safe_get(interface_row, 'type'))
    if int_type and 'sql' in int_type:
        if (sql_file and 'sql' in sql_file) or (queryname and 'query' in queryname):
            score += 5
            details.append(f"‚úì Type indicator: SQL")
    
    return score, details, 0  # semantic_score returned as 0 (calculated separately)

# ========================================================
# FILTERING
# ========================================================

def apply_global_filters(df):
    """Apply global filters"""
    if df is None or df.empty:
        return df
    
    filtered = df.copy()
    
    if st.session_state.filter_source_systems and 'source_system' in filtered.columns:
        filtered = filtered[filtered['source_system'].isin(st.session_state.filter_source_systems)]
    
    if st.session_state.filter_target_systems and 'target_system' in filtered.columns:
        filtered = filtered[filtered['target_system'].isin(st.session_state.filter_target_systems)]
    
    if st.session_state.filter_types and 'type' in filtered.columns:
        filtered = filtered[filtered['type'].isin(st.session_state.filter_types)]
    
    if st.session_state.filter_applications and 'application' in filtered.columns:
        filtered = filtered[filtered['application'].isin(st.session_state.filter_applications)]
    
    return filtered

def get_active_filter_count():
    """Count active filters"""
    count = 0
    count += len(st.session_state.filter_source_systems)
    count += len(st.session_state.filter_target_systems)
    count += len(st.session_state.filter_types)
    count += len(st.session_state.filter_applications)
    return count

def clear_all_filters():
    """Clear all filters"""
    st.session_state.filter_source_systems = []
    st.session_state.filter_target_systems = []
    st.session_state.filter_types = []
    st.session_state.filter_applications = []

# Continue in next message due to length...

# ========================================================
# NAVIGATION
# ========================================================

def navigate_to_overview():
    st.session_state.navigation_level = 'overview'
    st.session_state.selected_source = None
    st.session_state.selected_target = None
    st.session_state.selected_interface = None
    st.rerun()

def navigate_to_system_pair(source, target):
    st.session_state.navigation_level = 'system_pair'
    st.session_state.selected_source = source
    st.session_state.selected_target = target
    st.session_state.selected_interface = None
    st.rerun()

def navigate_to_interface(interface_name):
    st.session_state.navigation_level = 'interface_detail'
    st.session_state.selected_interface = interface_name
    st.rerun()

def navigate_to_sql_detail(query_name):
    st.session_state.navigation_level = 'sql_detail'
    st.session_state.selected_sql = query_name
    st.rerun()

# ========================================================
# VIEWS (Simplified for length - same as before)
# ========================================================

def render_overview():
    st.markdown('<div class="premium-header"><h1 class="premium-title">üåê Data Explorer 360¬∞</h1><p class="premium-subtitle">Enhanced Semantic Matching</p></div>', unsafe_allow_html=True)
    
    df = apply_global_filters(st.session_state.interface_df)
    
    if df.empty:
        st.warning("‚ö†Ô∏è No data matches current filters.")
        return
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown(f'<div class="metric-card"><div class="metric-value">{len(df)}</div><div class="metric-label">Interfaces</div></div>', unsafe_allow_html=True)
    
    with col2:
        systems = df['source_system'].nunique()
        st.markdown(f'<div class="metric-card"><div class="metric-value">{systems}</div><div class="metric-label">Systems</div></div>', unsafe_allow_html=True)
    
    with col3:
        if st.session_state.sql_df is not None:
            st.markdown(f'<div class="metric-card"><div class="metric-value">{len(st.session_state.sql_df)}</div><div class="metric-label">SQL Queries</div></div>', unsafe_allow_html=True)
    
    with col4:
        if st.session_state.mapping_df is not None:
            mapping_filtered = apply_global_filters(st.session_state.mapping_df)
            st.markdown(f'<div class="metric-card"><div class="metric-value">{len(mapping_filtered)}</div><div class="metric-label">Mappings</div></div>', unsafe_allow_html=True)
    
    st.markdown("---")
    st.subheader("üîó System Connections")
    
    system_pairs = df.groupby(['source_system', 'target_system']).size().reset_index(name='count')
    system_pairs = system_pairs.sort_values('count', ascending=False)
    
    for _, row in system_pairs.iterrows():
        source = row['source_system']
        target = row['target_system']
        count = row['count']
        
        col1, col2 = st.columns([4, 1])
        
        with col1:
            st.markdown(f'<div class="interactive-card"><div class="card-title">{source} ‚Üí {target}</div><div class="card-subtitle">{count} interfaces</div></div>', unsafe_allow_html=True)
        
        with col2:
            if st.button("Drill Down ‚Üí", key=f"drill_{source}_{target}"):
                navigate_to_system_pair(source, target)

def render_system_pair():
    source = st.session_state.selected_source
    target = st.session_state.selected_target
    
    col1, col2 = st.columns([6, 1])
    with col1:
        st.markdown(f'<div class="breadcrumb">üè† Overview ‚Üí üîó {source} ‚Üí {target}</div>', unsafe_allow_html=True)
    with col2:
        if st.button("‚Üê Back"):
            navigate_to_overview()
    
    st.markdown(f'<h1 style="color: #667eea;">üîó {source} ‚Üí {target}</h1>', unsafe_allow_html=True)
    
    df = apply_global_filters(st.session_state.interface_df)
    filtered = df[(df['source_system'] == source) & (df['target_system'] == target)]
    
    if filtered.empty:
        st.warning("‚ö†Ô∏è No interfaces found for this system pair with current filters.")
        return
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Interfaces", len(filtered))
    with col2:
        if 'application' in filtered.columns:
            apps = filtered['application'].nunique()
            st.metric("Applications", apps)
    with col3:
        if 'type' in filtered.columns:
            types = filtered['type'].nunique()
            st.metric("Types", types)
    
    st.markdown("---")
    st.subheader("üìã Interfaces")
    
    for _, row in filtered.iterrows():
        integration = safe_get(row, 'integration')
        application = safe_get(row, 'application')
        desc = safe_get(row, 'description')
        type_val = safe_get(row, 'type')
        
        col1, col2 = st.columns([5, 1])
        
        with col1:
            st.markdown(f'<div class="interactive-card"><div class="card-title">{integration}</div><div class="card-subtitle">App: {application} | {desc[:80]}...</div><div style="margin-top: 0.5rem; color: #666;">Type: {type_val}</div></div>', unsafe_allow_html=True)
        
        with col2:
            if st.button("Details ‚Üí", key=f"int_{integration}"):
                navigate_to_interface(integration)

def render_interface_detail():
    interface = st.session_state.selected_interface
    source = st.session_state.selected_source
    target = st.session_state.selected_target
    
    col1, col2 = st.columns([6, 1])
    with col1:
        st.markdown(f'<div class="breadcrumb">üè† ‚Üí üîó {source} ‚Üí {target} ‚Üí üìã {interface}</div>', unsafe_allow_html=True)
    with col2:
        if st.button("‚Üê Back"):
            navigate_to_system_pair(source, target)
    
    st.markdown(f'<h1 style="color: #667eea;">üìã {interface}</h1>', unsafe_allow_html=True)
    
    df = st.session_state.interface_df
    int_row = df[df['integration'] == interface].iloc[0]
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Interface Information")
        st.write(f"**Application:** {safe_get(int_row, 'application')}")
        st.write(f"**Integration:** {safe_get(int_row, 'integration')}")
        st.write(f"**Description:** {safe_get(int_row, 'description')}")
        st.write(f"**Type:** {safe_get(int_row, 'type')}")
    
    with col2:
        st.subheader("Connection Details")
        st.write(f"**Source:** {safe_get(int_row, 'source_system')}")
        st.write(f"**Target:** {safe_get(int_row, 'target_system')}")
        st.write(f"**Direction:** {safe_get(int_row, 'inbound_outbound')}")
    
    st.markdown("---")
    
    if st.session_state.mapping_df is not None and not st.session_state.mapping_df.empty:
        if 'integration' in st.session_state.mapping_df.columns:
            matched_sql = st.session_state.mapping_df[
                st.session_state.mapping_df['integration'] == interface
            ]
        else:
            st.error("‚ö†Ô∏è Please regenerate mapping with updated code")
            matched_sql = pd.DataFrame()
        
        if not matched_sql.empty:
            st.subheader("üíª Matching SQL Queries")
            st.write(f"Found {len(matched_sql)} matches")
            
            for _, sql_row in matched_sql.iterrows():
                query = safe_get(sql_row, 'queryname')
                score = safe_get(sql_row, 'final_score')
                confidence = safe_get(sql_row, 'confidence')
                tables = safe_get(sql_row, 'tables')
                match_details = safe_get(sql_row, 'match_details')
                
                conf_class = f"confidence-{confidence.lower()}"
                
                col1, col2 = st.columns([5, 1])
                
                with col1:
                    st.markdown(f'''
                    <div class="interactive-card">
                        <div class="card-title">{query}</div>
                        <div class="card-subtitle">Tables: {tables}</div>
                        <div style="margin-top: 0.5rem;">
                            <span class="{conf_class}">{confidence} ({score}%)</span>
                        </div>
                        <div style="margin-top: 0.5rem; font-size: 0.85rem; color: #666;">
                            {match_details}
                        </div>
                    </div>
                    ''', unsafe_allow_html=True)
                
                with col2:
                    if st.button("SQL ‚Üí", key=f"sql_{query}"):
                        navigate_to_sql_detail(query)
        else:
            st.info("No matching SQL queries found.")
    else:
        st.info("Generate mapping to see matched queries.")

def render_sql_detail():
    """Render SQL detail view with business and technical explanations"""
    query_name = st.session_state.selected_sql
    interface = st.session_state.selected_interface
    
    col1, col2 = st.columns([6, 1])
    with col1:
        st.markdown(f'<div class="breadcrumb">üè† ‚Üí ... ‚Üí üìã {interface} ‚Üí üíª {query_name}</div>', unsafe_allow_html=True)
    with col2:
        if st.button("‚Üê Back"):
            navigate_to_interface(interface)
    
    st.markdown(f'<h1 style="color: #667eea;">üíª {query_name}</h1>', unsafe_allow_html=True)
    
    # Get SQL details
    sql_df = st.session_state.sql_df
    if sql_df is None or sql_df.empty:
        st.error("‚ö†Ô∏è SQL metadata not available")
        return
    
    sql_matches = sql_df[sql_df['queryname'] == query_name]
    if sql_matches.empty:
        st.error(f"‚ö†Ô∏è Query '{query_name}' not found")
        return
    
    sql_row = sql_matches.iloc[0]
    
    # Get interface details for context
    int_df = st.session_state.interface_df
    int_row = int_df[int_df['integration'] == interface].iloc[0] if interface in int_df['integration'].values else None
    
    # Extract data
    raw_sql = safe_get(sql_row, 'rawsql', 'SQL code not available')
    tables_str = safe_get(sql_row, 'tables', '')
    
    # Parse tables
    if tables_str:
        tables_list = [t.strip() for t in str(tables_str).split(',')]
    else:
        tables_list = []
    
    # Analyze SQL
    analysis = analyze_sql_query(raw_sql)
    if not analysis['tables'] and tables_list:
        analysis['tables'] = tables_list
    
    # Tabs for different views
    tab1, tab2, tab3, tab4 = st.tabs(["üíª SQL Code", "üìä Analysis", "ü§ñ Business Insights", "‚öôÔ∏è Technical Details"])
    
    with tab1:
        st.subheader("SQL Query Code")
        st.code(raw_sql, language="sql")
        
        st.markdown("---")
        col1, col2 = st.columns(2)
        with col1:
            st.write(f"**System:** {safe_get(sql_row, 'system')}")
            st.write(f"**File:** {safe_get(sql_row, 'file')}")
        with col2:
            st.write(f"**Query Name:** {query_name}")
            st.write(f"**Complexity:** {analysis['complexity']}")
    
    with tab2:
        st.subheader("Query Analysis")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Tables", len(analysis['tables']))
        with col2:
            st.metric("Joins", len(analysis['joins']))
        with col3:
            st.metric("Subqueries", "Yes" if analysis['has_subquery'] else "No")
        with col4:
            st.metric("Complexity", analysis['complexity'])
        
        st.markdown("---")
        
        if analysis['tables']:
            st.subheader("üìä Tables Used")
            for i, table in enumerate(analysis['tables'], 1):
                st.markdown(f'<span class="column-badge">{i}. {table}</span>', unsafe_allow_html=True)
        
        if analysis['has_group_by']:
            st.info("‚ÑπÔ∏è This query includes aggregation (GROUP BY)")
        
        if analysis['has_subquery']:
            st.info("‚ÑπÔ∏è This query uses subqueries for complex filtering")
        
        if len(analysis['joins']) > 3:
            st.warning(f"‚ö†Ô∏è This query has {len(analysis['joins'])} joins - ensure proper indexing")
    
    with tab3:
        st.subheader("ü§ñ Business Explanation")
        
        st.markdown('<div class="ai-insight">', unsafe_allow_html=True)
        
        if int_row is not None:
            business_exp = generate_business_explanation_rules(int_row, sql_row, analysis['tables'])
        else:
            # Fallback if no interface context
            business_exp = f"""**Business Purpose:**

This query ({query_name}) retrieves data from {len(analysis['tables'])} table(s) in the {safe_get(sql_row, 'system', 'system')}.

**Data Scope:**

The query accesses: {', '.join(analysis['tables'][:5])}

**Complexity:** {analysis['complexity']} - """
            if analysis['complexity'] == 'High':
                business_exp += "This is a comprehensive data extraction that combines multiple data sources."
            elif analysis['complexity'] == 'Medium':
                business_exp += "This retrieves related data from multiple tables."
            else:
                business_exp += "This is a straightforward data extraction."
        
        st.markdown(business_exp)
        st.markdown('</div>', unsafe_allow_html=True)
        
        st.info("üí° These insights are generated using rule-based analysis (no GPU required)")
    
    with tab4:
        st.subheader("‚öôÔ∏è Technical Explanation")
        
        st.markdown('<div class="ai-insight" style="background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%); border-left-color: #2196f3;">', unsafe_allow_html=True)
        
        technical_exp = generate_technical_explanation_rules(sql_row, analysis['tables'], analysis)
        st.markdown(technical_exp)
        
        st.markdown('</div>', unsafe_allow_html=True)
        
        # Additional technical details
        st.markdown("---")
        st.subheader("üìã Query Metadata")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write(f"**System:** {safe_get(sql_row, 'system')}")
            st.write(f"**File Path:** {safe_get(sql_row, 'file')}")
            st.write(f"**Query Name:** {query_name}")
        
        with col2:
            st.write(f"**Tables Accessed:** {len(analysis['tables'])}")
            st.write(f"**Join Operations:** {len(analysis['joins'])}")
            st.write(f"**Complexity Level:** {analysis['complexity']}")
        
        if analysis['has_subquery']:
            st.write("**Contains Subqueries:** Yes")
        if analysis['has_group_by']:
            st.write("**Aggregation:** Yes (GROUP BY)")
        if analysis['has_union']:
            st.write("**Union Operations:** Yes")
        
        st.info("üí° Technical analysis uses lightweight pattern matching (CPU-optimized)")



def main():
    st.sidebar.title("üåê Data Explorer 360¬∞")
    st.sidebar.markdown("### Enhanced Semantic Matching")
    st.sidebar.markdown("---")
    
    # Upload section
    with st.sidebar.expander("üìÇ Upload Data", expanded=st.session_state.interface_df is None):
        interface_file = st.file_uploader("Interface Inventory", type=["xlsx", "xls"])
        
        if interface_file:
            with st.spinner("Loading interface data..."):
                st.session_state.interface_df = load_interface_inventory(interface_file)
            
            if st.session_state.interface_df is not None:
                st.success(f"‚úÖ Loaded {len(st.session_state.interface_df)} interfaces")
        
        sql_file = st.file_uploader("SQL Metadata", type=["xlsx", "xls"])
        
        if sql_file:
            with st.spinner("Loading SQL data..."):
                st.session_state.sql_df = load_sql_metadata(sql_file)
            
            if st.session_state.sql_df is not None:
                st.success(f"‚úÖ Loaded {len(st.session_state.sql_df)} queries")
        
        # Load existing mapping
        if os.path.exists(MAPPING_FILE) and st.session_state.mapping_df is None:
            try:
                st.session_state.mapping_df = pd.read_excel(MAPPING_FILE)
                st.info(f"üìÑ Loaded existing mapping ({len(st.session_state.mapping_df)} records)")
            except:
                pass
        
        if st.session_state.interface_df is not None and st.session_state.sql_df is not None:
            st.markdown("### üéØ Matching Settings")
            
            use_semantic = st.checkbox(
                "Enable Semantic Matching",
                value=True if embedding_model else False,
                disabled=not embedding_model,
                help="Uses AI to find similar interfaces and SQL queries"
            )
            
            min_score = st.slider(
                "Minimum Confidence Score",
                30, 100, 45, 5,
                help="Lower = more matches but less confident"
            )
            
            if st.button("üîó Generate Mapping", use_container_width=True):
                with st.spinner("Generating enhanced semantic mapping..."):
                    st.session_state.mapping_df = generate_enhanced_mapping(
                        st.session_state.interface_df,
                        st.session_state.sql_df,
                        min_score=min_score,
                        use_semantic=use_semantic
                    )
                    
                    if not st.session_state.mapping_df.empty:
                        try:
                            st.session_state.mapping_df.to_excel(MAPPING_FILE, index=False)
                            st.success(f"‚úÖ Generated {len(st.session_state.mapping_df)} mappings")
                            st.info(f"üìÑ Saved to: {os.path.abspath(MAPPING_FILE)}")
                        except Exception as e:
                            st.success(f"‚úÖ Generated {len(st.session_state.mapping_df)} mappings")
                            st.warning(f"‚ö†Ô∏è Could not save: {e}")
                    else:
                        st.warning("‚ö†Ô∏è No mappings found. Try lowering minimum score.")
                st.rerun()
    
    # GLOBAL FILTERS
    if st.session_state.interface_df is not None:
        st.sidebar.markdown("---")
        st.sidebar.subheader("üîç Global Filters")
        
        df = st.session_state.interface_df
        
        if 'source_system' in df.columns:
            sources = sorted(df['source_system'].dropna().unique())
            st.session_state.filter_source_systems = st.sidebar.multiselect(
                "Source System",
                options=sources,
                default=st.session_state.filter_source_systems
            )
        
        if 'target_system' in df.columns:
            targets = sorted(df['target_system'].dropna().unique())
            st.session_state.filter_target_systems = st.sidebar.multiselect(
                "Target System",
                options=targets,
                default=st.session_state.filter_target_systems
            )
        
        if 'type' in df.columns:
            types = sorted(df['type'].dropna().unique())
            st.session_state.filter_types = st.sidebar.multiselect(
                "Type",
                options=types,
                default=st.session_state.filter_types
            )
        
        if 'application' in df.columns:
            apps = sorted(df['application'].dropna().unique())
            st.session_state.filter_applications = st.sidebar.multiselect(
                "Application",
                options=apps,
                default=st.session_state.filter_applications
            )
        
        if get_active_filter_count() > 0:
            if st.sidebar.button("üóëÔ∏è Clear All Filters", use_container_width=True):
                clear_all_filters()
                st.rerun()
    
    # Navigation
    st.sidebar.markdown("---")
    st.sidebar.subheader("üìç Current Location")
    
    level = st.session_state.navigation_level
    
    if level == 'overview':
        st.sidebar.write("üè† 360¬∞ Overview")
    elif level == 'system_pair':
        st.sidebar.write(f"üîó {st.session_state.selected_source} ‚Üí {st.session_state.selected_target}")
    elif level == 'interface_detail':
        st.sidebar.write(f"üìã {st.session_state.selected_interface}")
    
    if level != 'overview':
        if st.sidebar.button("üè† Return to Overview", use_container_width=True):
            navigate_to_overview()
    
    # Main content
    if st.session_state.interface_df is None:
        st.markdown('<div class="premium-header"><h1 class="premium-title">üåê Welcome to Data Explorer 360¬∞</h1><p class="premium-subtitle">Enhanced Semantic Matching Edition</p></div>', unsafe_allow_html=True)
        
        st.info("üëà Upload your Interface Inventory and SQL Metadata in the sidebar")
        
        st.markdown("""
        ### ‚ú® Enhanced Matching Features
        
        This version uses **intelligent semantic matching** to find relationships between:
        
        **Interface Side:** Application, Integration, Description, Source System, Type, Target System
        
        **SQL Side:** System, File, QueryName, Tables
        
        **Matching Strategy:**
        - üéØ **Deterministic (80%)**: Exact matches on applications, integrations, descriptions
        - ü§ñ **Semantic (50%)**: AI-powered similarity on descriptions and context
        
        **Result:** More accurate matches with confidence scores!
        
        ### üíª CPU-Optimized
        
        - ‚úÖ Lightweight embedding model (500MB RAM)
        - ‚úÖ Rule-based explanations (no GPU needed)
        - ‚úÖ Fast pattern-based SQL analysis
        - ‚úÖ Works great on standard laptops!
        """)
    else:
        if level == 'overview':
            render_overview()
        elif level == 'system_pair':
            render_system_pair()
        elif level == 'interface_detail':
            render_interface_detail()
        elif level == 'sql_detail':
            render_sql_detail()

if __name__ == "__main__":
    main()

