"""
========================================================
Enterprise Data Explorer 360¬∞ - Enhanced Semantic Matching
Optimized for your specific data structure
========================================================
"""

import streamlit as st
import pandas as pd
import json
import re
import os
from collections import defaultdict
import numpy as np

# ML/NLP
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
    from sentence_transformers import SentenceTransformer, util
    ML_AVAILABLE = True
except:
    ML_AVAILABLE = False
    st.warning("‚ö†Ô∏è ML libraries not available. Semantic matching will be limited.")

# SQL Parsing
try:
    import sqlglot
    from sqlglot.expressions import Table, Column, Join
    SQLGLOT_AVAILABLE = True
except:
    SQLGLOT_AVAILABLE = False

# Visualization  
try:
    from streamlit_agraph import agraph, Node, Edge, Config
    AGRAPH_AVAILABLE = True
except:
    AGRAPH_AVAILABLE = False

try:
    from st_aggrid import AgGrid, GridOptionsBuilder, GridUpdateMode
    AGGRID_AVAILABLE = True
except:
    AGGRID_AVAILABLE = False

# Configuration
st.set_page_config(
    page_title="Data Explorer 360¬∞ Pro",
    page_icon="üåê",
    layout="wide",
    initial_sidebar_state="expanded"
)

if ML_AVAILABLE:
    torch.set_num_threads(8)

MAPPING_FILE = "interface_sql_mapping.xlsx"

# ========================================================
# PREMIUM STYLING
# ========================================================

st.markdown("""
<style>
    .premium-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 50%, #f093fb 100%);
        padding: 2rem;
        border-radius: 15px;
        text-align: center;
        margin-bottom: 2rem;
        box-shadow: 0 10px 30px rgba(102, 126, 234, 0.3);
    }
    
    .premium-title {
        font-size: 3.5rem;
        font-weight: 800;
        color: white;
        margin: 0;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
    }
    
    .breadcrumb {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem 2rem;
        border-radius: 12px;
        margin-bottom: 2rem;
        color: white;
        font-size: 1.1rem;
    }
    
    .metric-card {
        background: white;
        padding: 2rem;
        border-radius: 15px;
        box-shadow: 0 5px 20px rgba(0,0,0,0.08);
        text-align: center;
        transition: all 0.3s ease;
        border-top: 4px solid #667eea;
    }
    
    .metric-card:hover {
        transform: translateY(-8px);
        box-shadow: 0 10px 30px rgba(102, 126, 234, 0.2);
    }
    
    .metric-value {
        font-size: 3rem;
        font-weight: 800;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    
    .interactive-card {
        background: white;
        padding: 1.5rem;
        border-radius: 12px;
        border-left: 5px solid #667eea;
        margin: 1rem 0;
        box-shadow: 0 3px 15px rgba(0,0,0,0.08);
        cursor: pointer;
        transition: all 0.3s ease;
    }
    
    .interactive-card:hover {
        transform: translateX(10px);
        box-shadow: 0 5px 25px rgba(102, 126, 234, 0.15);
    }
    
    .ai-insight {
        background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
        border-radius: 12px;
        padding: 2rem;
        margin: 2rem 0;
        border-left: 6px solid #ff6b6b;
    }
    
    .confidence-high {
        background: #e8f5e9;
        color: #2e7d32;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-weight: 700;
    }
    
    .confidence-medium {
        background: #fff3e0;
        color: #ef6c00;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-weight: 700;
    }
    
    .confidence-low {
        background: #fce4ec;
        color: #c2185b;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-weight: 700;
    }
    
    .column-badge {
        display: inline-block;
        background: #e3f2fd;
        color: #1976d2;
        padding: 0.4rem 1rem;
        border-radius: 15px;
        margin: 0.3rem;
        font-size: 0.9rem;
    }
    
    .active-filter-panel {
        background: #fff3e0;
        border-left: 4px solid #ff9800;
        padding: 1rem;
        border-radius: 8px;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# ========================================================
# SESSION STATE
# ========================================================

if 'navigation_level' not in st.session_state:
    st.session_state.navigation_level = 'overview'
if 'selected_source' not in st.session_state:
    st.session_state.selected_source = None
if 'selected_target' not in st.session_state:
    st.session_state.selected_target = None  
if 'selected_interface' not in st.session_state:
    st.session_state.selected_interface = None
if 'selected_sql' not in st.session_state:
    st.session_state.selected_sql = None

# Data
if 'interface_df' not in st.session_state:
    st.session_state.interface_df = None
if 'sql_df' not in st.session_state:
    st.session_state.sql_df = None
if 'mapping_df' not in st.session_state:
    st.session_state.mapping_df = None

# GLOBAL FILTERS
if 'filter_source_systems' not in st.session_state:
    st.session_state.filter_source_systems = []
if 'filter_target_systems' not in st.session_state:
    st.session_state.filter_target_systems = []
if 'filter_types' not in st.session_state:
    st.session_state.filter_types = []
if 'filter_applications' not in st.session_state:
    st.session_state.filter_applications = []

# ========================================================
# LOAD MODELS
# ========================================================

@st.cache_resource(show_spinner=False)
def load_models():
    embedding = None
    llm = None
    
    if ML_AVAILABLE:
        try:
            embedding = SentenceTransformer("all-MiniLM-L6-v2")
            st.success("‚úÖ Embedding model loaded - Semantic matching enabled!")
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Could not load embedding model: {e}")
    
    return embedding, llm

embedding_model, llm_model = load_models()

# ========================================================
# UTILITY FUNCTIONS
# ========================================================

def clean_column_name(col):
    """Clean column names"""
    return str(col).strip().lower().replace(" ", "_").replace("/", "_").replace("-", "_").replace("(", "").replace(")", "").strip("_")

def safe_get(row, column, default=""):
    """Safely get value from row"""
    try:
        val = row.get(column, default) if isinstance(row, dict) else getattr(row, column, default)
        return val if pd.notna(val) and str(val) != 'nan' and str(val) != '' else default
    except:
        return default

def normalize_text(text):
    """Normalize text for matching"""
    if not isinstance(text, str):
        return ""
    return text.lower().replace("_", "").replace(" ", "").replace("-", "").strip()

# ========================================================
# DATA LOADING - YOUR SPECIFIC STRUCTURE
# ========================================================

def load_interface_inventory(file):
    """Load interface inventory - YOUR SPECIFIC COLUMNS"""
    
    st.info("üîç Loading Interface Inventory with your column structure...")
    
    # Try different sheet names
    for sheet in ["interface", "Interface", "Sheet1", "Interfaces"]:
        try:
            df = pd.read_excel(file, sheet_name=sheet)
            st.success(f"‚úÖ Found sheet: '{sheet}'")
            break
        except:
            continue
    else:
        df = pd.read_excel(file)
        st.info("‚úÖ Loaded from first sheet")
    
    # Show original columns
    with st.expander("üìã Original Columns Detected"):
        st.write(list(df.columns))
    
    # Clean column names
    df.columns = [clean_column_name(col) for col in df.columns]
    
    # Remove empty columns
    df = df.dropna(axis=1, how='all')
    df = df.loc[:, ~df.columns.str.contains("^unnamed", case=False)]
    
    # Map YOUR specific columns
    column_mappings = {
        'application': ['application', 'app'],
        'integration': ['integration', 'interface'],
        'description': ['description', 'desc'],
        'type': ['type'],
        'source_system': ['source_system', 'source'],
        'target_system': ['target_system', 'target'],
        'inbound_outbound': ['inbound_outbound_with_respect_to_existing_acct_platform', 'inbound_outbound', 'direction'],
        'feed_routing': ['feed_routing', 'routing'],
        'frequency': ['frequency'],
        'owner': ['application_owner_contact', 'owner', 'contact']
    }
    
    for std_name, variations in column_mappings.items():
        for var in variations:
            if var in df.columns and std_name not in df.columns:
                df[std_name] = df[var]
                break
    
    # Ensure minimum required columns
    required = ['application', 'integration', 'source_system', 'target_system']
    missing = [col for col in required if col not in df.columns]
    
    if missing:
        st.error(f"‚ùå Missing required columns: {', '.join(missing)}")
        st.write("Available columns:", list(df.columns))
        return None
    
    # Clean strings
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = df[col].astype(str).str.strip().replace({'nan': '', 'None': '', 'NaN': ''})
    
    # Show mapped columns
    with st.expander("‚úÖ Mapped Columns"):
        st.write({k: v for k, v in column_mappings.items() if k in df.columns})
    
    return df

def load_sql_metadata(file):
    """Load SQL metadata - YOUR SPECIFIC COLUMNS"""
    
    st.info("üîç Loading SQL Metadata with your column structure...")
    
    for sheet in ["Queries", "Sheet1", "SQL", "Output"]:
        try:
            df = pd.read_excel(file, sheet_name=sheet)
            st.success(f"‚úÖ Found sheet: '{sheet}'")
            break
        except:
            continue
    else:
        df = pd.read_excel(file)
        st.info("‚úÖ Loaded from first sheet")
    
    # Show original columns
    with st.expander("üìã SQL Original Columns"):
        st.write(list(df.columns))
    
    # Clean columns
    df.columns = [clean_column_name(col) for col in df.columns]
    
    # Map columns
    mappings = {
        'system': ['system'],
        'file': ['file'],
        'queryname': ['queryname', 'query_name', 'query'],
        'tables': ['tables', 'table']
    }
    
    for std, vars in mappings.items():
        for v in vars:
            if v in df.columns and std not in df.columns:
                df[std] = df[v]
                break
    
    # Check required
    required = ['queryname']
    missing = [col for col in required if col not in df.columns]
    
    if missing:
        st.error(f"‚ùå Missing required SQL columns: {', '.join(missing)}")
        return None
    
    # Clean strings
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = df[col].astype(str).str.strip().replace({'nan': '', 'None': ''})
    
    return df

# ========================================================
# ENHANCED SEMANTIC MATCHING ENGINE
# ========================================================

def calculate_enhanced_match_score(interface_row, sql_row, use_semantic=True):
    """
    OPTIMIZED MATCHING - HIGHEST PRIORITY:
    Interface: Description, Application, Integration
    SQL: File, QueryName
    """
    
    score = 0
    details = []
    
    # Extract key fields
    application = normalize_text(safe_get(interface_row, 'application'))
    integration = normalize_text(safe_get(interface_row, 'integration'))
    description = normalize_text(safe_get(interface_row, 'description'))
    
    queryname = normalize_text(safe_get(sql_row, 'queryname'))
    sql_file = normalize_text(safe_get(sql_row, 'file'))
    
    # === PRIORITY MATCHING: Description/Application/Integration vs File/QueryName ===
    # Total: 80 points for primary matching
    
    # 1. APPLICATION MATCHING (30 points)
    if application:
        # Application in QueryName (20 points)
        if queryname and (application in queryname or queryname in application):
            score += 20
            details.append(f"‚úì Application '{application}' in QueryName")
        elif queryname:
            # Partial word match
            app_words = set(application.split())
            query_words = set(queryname.split())
            common = app_words & query_words
            if common:
                score += min(len(common) * 5, 15)
                details.append(f"‚úì Application words in QueryName ({len(common)})")
        
        # Application in File (10 points)
        if sql_file and (application in sql_file or sql_file in application):
            score += 10
            details.append(f"‚úì Application '{application}' in File")
        elif sql_file:
            # Partial word match
            file_words = set(sql_file.split())
            common = app_words & file_words
            if common:
                score += min(len(common) * 3, 8)
                details.append(f"‚úì Application words in File ({len(common)})")
    
    # 2. INTEGRATION MATCHING (30 points)
    if integration:
        # Integration in QueryName (20 points)
        if queryname and (integration in queryname or queryname in integration):
            score += 20
            details.append(f"‚úì Integration '{integration}' in QueryName")
        elif queryname:
            # Word-level matching
            int_words = set(integration.split())
            query_words = set(queryname.split())
            common = int_words & query_words
            if common:
                score += min(len(common) * 5, 15)
                details.append(f"‚úì Integration words in QueryName ({len(common)})")
        
        # Integration in File (10 points)
        if sql_file and (integration in sql_file or sql_file in integration):
            score += 10
            details.append(f"‚úì Integration '{integration}' in File")
        elif sql_file:
            # Partial word match
            file_words = set(sql_file.split())
            common = int_words & file_words
            if common:
                score += min(len(common) * 3, 8)
                details.append(f"‚úì Integration words in File ({len(common)})")
    
    # 3. DESCRIPTION MATCHING (20 points)
    if description:
        desc_words = set(description.split())
        
        # Description keywords in QueryName (10 points)
        if queryname:
            query_words = set(queryname.split())
            common = desc_words & query_words
            if common:
                score += min(len(common) * 2, 10)
                details.append(f"‚úì Description words in QueryName ({len(common)})")
        
        # Description keywords in File (10 points)
        if sql_file:
            file_words = set(sql_file.split())
            common = desc_words & file_words
            if common:
                score += min(len(common) * 2, 10)
                details.append(f"‚úì Description words in File ({len(common)})")
    
    # === SECONDARY MATCHING: System/Type ===
    # Total: 20 points for supporting evidence
    
    # 4. System Matching (15 points)
    source_sys = normalize_text(safe_get(interface_row, 'source_system'))
    sql_sys = normalize_text(safe_get(sql_row, 'system'))
    
    if source_sys and sql_sys:
        if source_sys == sql_sys:
            score += 15
            details.append(f"‚úì System match: {source_sys}")
        elif source_sys in sql_sys or sql_sys in source_sys:
            score += 10
            details.append(f"‚úì Partial system match")
    
    # 5. Type/Format Hints (5 points)
    int_type = normalize_text(safe_get(interface_row, 'type'))
    if int_type and 'sql' in int_type:
        if (sql_file and 'sql' in sql_file) or (queryname and 'query' in queryname):
            score += 5
            details.append(f"‚úì Type indicator: SQL")
    
    # === SEMANTIC BOOST (FOCUSED ON KEY FIELDS) ===
    # Use ONLY the priority fields for semantic matching
    
    semantic_score = 0
    if use_semantic and embedding_model:
        try:
            # Build focused text - ONLY priority fields
            interface_text = f"{application} {integration} {description}".strip()
            sql_text = f"{queryname} {sql_file}".strip()
            
            if interface_text and sql_text:
                # Calculate semantic similarity
                int_emb = embedding_model.encode(interface_text, convert_to_tensor=True)
                sql_emb = embedding_model.encode(sql_text, convert_to_tensor=True)
                
                similarity = util.cos_sim(int_emb, sql_emb).item()
                
                # Semantic score (0-50 points for very high similarity)
                if similarity >= 0.85:
                    semantic_score = 50
                    details.append(f"ü§ñ Excellent semantic match ({similarity:.2f})")
                elif similarity >= 0.75:
                    semantic_score = 40
                    details.append(f"ü§ñ Very high semantic match ({similarity:.2f})")
                elif similarity >= 0.65:
                    semantic_score = 30
                    details.append(f"ü§ñ High semantic match ({similarity:.2f})")
                elif similarity >= 0.55:
                    semantic_score = 20
                    details.append(f"ü§ñ Good semantic match ({similarity:.2f})")
                elif similarity >= 0.45:
                    semantic_score = 10
                    details.append(f"ü§ñ Moderate semantic match ({similarity:.2f})")
        except Exception as e:
            pass  # Silent fail for semantic errors
    
    final_score = min(score + semantic_score, 100)
    
    return final_score, details, semantic_score

def generate_enhanced_mapping(interface_df, sql_df, min_score=45, use_semantic=True):
    """Generate enhanced semantic mapping"""
    
    results = []
    
    st.write(f"üîÑ Starting mapping generation...")
    st.write(f"   Interfaces: {len(interface_df)}")
    st.write(f"   SQL Queries: {len(sql_df)}")
    st.write(f"   Semantic Matching: {'‚úÖ Enabled' if use_semantic and embedding_model else '‚ùå Disabled'}")
    st.write(f"   Minimum Score: {min_score}")
    
    progress = st.progress(0)
    status = st.empty()
    
    total = len(interface_df)
    matched_count = 0
    
    for idx, (_, i_row) in enumerate(interface_df.iterrows()):
        progress.progress((idx + 1) / total)
        status.text(f"Processing interface {idx + 1} of {total}... (Found {matched_count} matches so far)")
        
        for _, s_row in sql_df.iterrows():
            final_score, match_details, semantic_score = calculate_enhanced_match_score(
                i_row, s_row, use_semantic=use_semantic
            )
            
            if final_score >= min_score:
                matched_count += 1
                
                # Determine confidence
                if final_score >= 85:
                    confidence = "High"
                elif final_score >= 70:
                    confidence = "Medium"
                else:
                    confidence = "Low"
                
                results.append({
                    'application': safe_get(i_row, 'application'),
                    'integration': safe_get(i_row, 'integration'),
                    'description': safe_get(i_row, 'description'),
                    'type': safe_get(i_row, 'type'),
                    'source_system': safe_get(i_row, 'source_system'),
                    'target_system': safe_get(i_row, 'target_system'),
                    'inbound_outbound': safe_get(i_row, 'inbound_outbound'),
                    'sql_system': safe_get(s_row, 'system'),
                    'sql_file': safe_get(s_row, 'file'),
                    'queryname': safe_get(s_row, 'queryname'),
                    'tables': safe_get(s_row, 'tables'),
                    'deterministic_score': final_score - semantic_score,
                    'semantic_score': semantic_score,
                    'final_score': final_score,
                    'confidence': confidence,
                    'match_details': '; '.join(match_details)
                })
    
    progress.empty()
    status.empty()
    
    st.success(f"‚úÖ Mapping complete! Found {matched_count} matches")
    
    df = pd.DataFrame(results)
    
    # Ensure all required columns
    if df.empty:
        st.warning("‚ö†Ô∏è No matches found. Try lowering the minimum score.")
        df = pd.DataFrame(columns=[
            'application', 'integration', 'description', 'type',
            'source_system', 'target_system', 'inbound_outbound',
            'sql_system', 'sql_file', 'queryname', 'tables',
            'deterministic_score', 'semantic_score', 'final_score',
            'confidence', 'match_details'
        ])
    
    return df

# ========================================================
# FILTERING
# ========================================================

def apply_global_filters(df):
    """Apply global filters"""
    if df is None or df.empty:
        return df
    
    filtered = df.copy()
    
    if st.session_state.filter_source_systems and 'source_system' in filtered.columns:
        filtered = filtered[filtered['source_system'].isin(st.session_state.filter_source_systems)]
    
    if st.session_state.filter_target_systems and 'target_system' in filtered.columns:
        filtered = filtered[filtered['target_system'].isin(st.session_state.filter_target_systems)]
    
    if st.session_state.filter_types and 'type' in filtered.columns:
        filtered = filtered[filtered['type'].isin(st.session_state.filter_types)]
    
    if st.session_state.filter_applications and 'application' in filtered.columns:
        filtered = filtered[filtered['application'].isin(st.session_state.filter_applications)]
    
    return filtered

def get_active_filter_count():
    """Count active filters"""
    count = 0
    count += len(st.session_state.filter_source_systems)
    count += len(st.session_state.filter_target_systems)
    count += len(st.session_state.filter_types)
    count += len(st.session_state.filter_applications)
    return count

def clear_all_filters():
    """Clear all filters"""
    st.session_state.filter_source_systems = []
    st.session_state.filter_target_systems = []
    st.session_state.filter_types = []
    st.session_state.filter_applications = []

# Continue in next message due to length...

# ========================================================
# NAVIGATION
# ========================================================

def navigate_to_overview():
    st.session_state.navigation_level = 'overview'
    st.session_state.selected_source = None
    st.session_state.selected_target = None
    st.session_state.selected_interface = None
    st.rerun()

def navigate_to_system_pair(source, target):
    st.session_state.navigation_level = 'system_pair'
    st.session_state.selected_source = source
    st.session_state.selected_target = target
    st.session_state.selected_interface = None
    st.rerun()

def navigate_to_interface(interface_name):
    st.session_state.navigation_level = 'interface_detail'
    st.session_state.selected_interface = interface_name
    st.rerun()

def navigate_to_sql_detail(query_name):
    st.session_state.navigation_level = 'sql_detail'
    st.session_state.selected_sql = query_name
    st.rerun()

# ========================================================
# VIEWS (Simplified for length - same as before)
# ========================================================

def render_overview():
    st.markdown('<div class="premium-header"><h1 class="premium-title">üåê Data Explorer 360¬∞</h1><p class="premium-subtitle">Enhanced Semantic Matching</p></div>', unsafe_allow_html=True)
    
    df = apply_global_filters(st.session_state.interface_df)
    
    if df.empty:
        st.warning("‚ö†Ô∏è No data matches current filters.")
        return
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown(f'<div class="metric-card"><div class="metric-value">{len(df)}</div><div class="metric-label">Interfaces</div></div>', unsafe_allow_html=True)
    
    with col2:
        systems = df['source_system'].nunique()
        st.markdown(f'<div class="metric-card"><div class="metric-value">{systems}</div><div class="metric-label">Systems</div></div>', unsafe_allow_html=True)
    
    with col3:
        if st.session_state.sql_df is not None:
            st.markdown(f'<div class="metric-card"><div class="metric-value">{len(st.session_state.sql_df)}</div><div class="metric-label">SQL Queries</div></div>', unsafe_allow_html=True)
    
    with col4:
        if st.session_state.mapping_df is not None:
            mapping_filtered = apply_global_filters(st.session_state.mapping_df)
            st.markdown(f'<div class="metric-card"><div class="metric-value">{len(mapping_filtered)}</div><div class="metric-label">Mappings</div></div>', unsafe_allow_html=True)
    
    st.markdown("---")
    st.subheader("üîó System Connections")
    
    system_pairs = df.groupby(['source_system', 'target_system']).size().reset_index(name='count')
    system_pairs = system_pairs.sort_values('count', ascending=False)
    
    for _, row in system_pairs.iterrows():
        source = row['source_system']
        target = row['target_system']
        count = row['count']
        
        col1, col2 = st.columns([4, 1])
        
        with col1:
            st.markdown(f'<div class="interactive-card"><div class="card-title">{source} ‚Üí {target}</div><div class="card-subtitle">{count} interfaces</div></div>', unsafe_allow_html=True)
        
        with col2:
            if st.button("Drill Down ‚Üí", key=f"drill_{source}_{target}"):
                navigate_to_system_pair(source, target)

def render_system_pair():
    source = st.session_state.selected_source
    target = st.session_state.selected_target
    
    col1, col2 = st.columns([6, 1])
    with col1:
        st.markdown(f'<div class="breadcrumb">üè† Overview ‚Üí üîó {source} ‚Üí {target}</div>', unsafe_allow_html=True)
    with col2:
        if st.button("‚Üê Back"):
            navigate_to_overview()
    
    st.markdown(f'<h1 style="color: #667eea;">üîó {source} ‚Üí {target}</h1>', unsafe_allow_html=True)
    
    df = apply_global_filters(st.session_state.interface_df)
    filtered = df[(df['source_system'] == source) & (df['target_system'] == target)]
    
    if filtered.empty:
        st.warning("‚ö†Ô∏è No interfaces found for this system pair with current filters.")
        return
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Interfaces", len(filtered))
    with col2:
        if 'application' in filtered.columns:
            apps = filtered['application'].nunique()
            st.metric("Applications", apps)
    with col3:
        if 'type' in filtered.columns:
            types = filtered['type'].nunique()
            st.metric("Types", types)
    
    st.markdown("---")
    st.subheader("üìã Interfaces")
    
    for _, row in filtered.iterrows():
        integration = safe_get(row, 'integration')
        application = safe_get(row, 'application')
        desc = safe_get(row, 'description')
        type_val = safe_get(row, 'type')
        
        col1, col2 = st.columns([5, 1])
        
        with col1:
            st.markdown(f'<div class="interactive-card"><div class="card-title">{integration}</div><div class="card-subtitle">App: {application} | {desc[:80]}...</div><div style="margin-top: 0.5rem; color: #666;">Type: {type_val}</div></div>', unsafe_allow_html=True)
        
        with col2:
            if st.button("Details ‚Üí", key=f"int_{integration}"):
                navigate_to_interface(integration)

def render_interface_detail():
    interface = st.session_state.selected_interface
    source = st.session_state.selected_source
    target = st.session_state.selected_target
    
    col1, col2 = st.columns([6, 1])
    with col1:
        st.markdown(f'<div class="breadcrumb">üè† ‚Üí üîó {source} ‚Üí {target} ‚Üí üìã {interface}</div>', unsafe_allow_html=True)
    with col2:
        if st.button("‚Üê Back"):
            navigate_to_system_pair(source, target)
    
    st.markdown(f'<h1 style="color: #667eea;">üìã {interface}</h1>', unsafe_allow_html=True)
    
    df = st.session_state.interface_df
    int_row = df[df['integration'] == interface].iloc[0]
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Interface Information")
        st.write(f"**Application:** {safe_get(int_row, 'application')}")
        st.write(f"**Integration:** {safe_get(int_row, 'integration')}")
        st.write(f"**Description:** {safe_get(int_row, 'description')}")
        st.write(f"**Type:** {safe_get(int_row, 'type')}")
    
    with col2:
        st.subheader("Connection Details")
        st.write(f"**Source:** {safe_get(int_row, 'source_system')}")
        st.write(f"**Target:** {safe_get(int_row, 'target_system')}")
        st.write(f"**Direction:** {safe_get(int_row, 'inbound_outbound')}")
    
    st.markdown("---")
    
    if st.session_state.mapping_df is not None and not st.session_state.mapping_df.empty:
        if 'integration' in st.session_state.mapping_df.columns:
            matched_sql = st.session_state.mapping_df[
                st.session_state.mapping_df['integration'] == interface
            ]
        else:
            st.error("‚ö†Ô∏è Please regenerate mapping with updated code")
            matched_sql = pd.DataFrame()
        
        if not matched_sql.empty:
            st.subheader("üíª Matching SQL Queries")
            st.write(f"Found {len(matched_sql)} matches")
            
            for _, sql_row in matched_sql.iterrows():
                query = safe_get(sql_row, 'queryname')
                score = safe_get(sql_row, 'final_score')
                confidence = safe_get(sql_row, 'confidence')
                tables = safe_get(sql_row, 'tables')
                match_details = safe_get(sql_row, 'match_details')
                
                conf_class = f"confidence-{confidence.lower()}"
                
                col1, col2 = st.columns([5, 1])
                
                with col1:
                    st.markdown(f'''
                    <div class="interactive-card">
                        <div class="card-title">{query}</div>
                        <div class="card-subtitle">Tables: {tables}</div>
                        <div style="margin-top: 0.5rem;">
                            <span class="{conf_class}">{confidence} ({score}%)</span>
                        </div>
                        <div style="margin-top: 0.5rem; font-size: 0.85rem; color: #666;">
                            {match_details}
                        </div>
                    </div>
                    ''', unsafe_allow_html=True)
                
                with col2:
                    if st.button("SQL ‚Üí", key=f"sql_{query}"):
                        navigate_to_sql_detail(query)
        else:
            st.info("No matching SQL queries found.")
    else:
        st.info("Generate mapping to see matched queries.")

# ========================================================
# MAIN APP
# ========================================================

def main():
    st.sidebar.title("üåê Data Explorer 360¬∞")
    st.sidebar.markdown("### Enhanced Semantic Matching")
    st.sidebar.markdown("---")
    
    # Upload section
    with st.sidebar.expander("üìÇ Upload Data", expanded=st.session_state.interface_df is None):
        interface_file = st.file_uploader("Interface Inventory", type=["xlsx", "xls"])
        
        if interface_file:
            with st.spinner("Loading interface data..."):
                st.session_state.interface_df = load_interface_inventory(interface_file)
            
            if st.session_state.interface_df is not None:
                st.success(f"‚úÖ Loaded {len(st.session_state.interface_df)} interfaces")
        
        sql_file = st.file_uploader("SQL Metadata", type=["xlsx", "xls"])
        
        if sql_file:
            with st.spinner("Loading SQL data..."):
                st.session_state.sql_df = load_sql_metadata(sql_file)
            
            if st.session_state.sql_df is not None:
                st.success(f"‚úÖ Loaded {len(st.session_state.sql_df)} queries")
        
        # Load existing mapping
        if os.path.exists(MAPPING_FILE) and st.session_state.mapping_df is None:
            try:
                st.session_state.mapping_df = pd.read_excel(MAPPING_FILE)
                st.info(f"üìÑ Loaded existing mapping ({len(st.session_state.mapping_df)} records)")
            except:
                pass
        
        if st.session_state.interface_df is not None and st.session_state.sql_df is not None:
            st.markdown("### üéØ Matching Settings")
            
            use_semantic = st.checkbox(
                "Enable Semantic Matching",
                value=True if embedding_model else False,
                disabled=not embedding_model,
                help="Uses AI to find similar interfaces and SQL queries"
            )
            
            min_score = st.slider(
                "Minimum Confidence Score",
                30, 100, 45, 5,
                help="Lower = more matches but less confident"
            )
            
            if st.button("üîó Generate Mapping", use_container_width=True):
                with st.spinner("Generating enhanced semantic mapping..."):
                    st.session_state.mapping_df = generate_enhanced_mapping(
                        st.session_state.interface_df,
                        st.session_state.sql_df,
                        min_score=min_score,
                        use_semantic=use_semantic
                    )
                    
                    if not st.session_state.mapping_df.empty:
                        try:
                            st.session_state.mapping_df.to_excel(MAPPING_FILE, index=False)
                            st.success(f"‚úÖ Generated {len(st.session_state.mapping_df)} mappings")
                            st.info(f"üìÑ Saved to: {os.path.abspath(MAPPING_FILE)}")
                        except Exception as e:
                            st.success(f"‚úÖ Generated {len(st.session_state.mapping_df)} mappings")
                            st.warning(f"‚ö†Ô∏è Could not save: {e}")
                    else:
                        st.warning("‚ö†Ô∏è No mappings found. Try lowering minimum score.")
                st.rerun()
    
    # GLOBAL FILTERS
    if st.session_state.interface_df is not None:
        st.sidebar.markdown("---")
        st.sidebar.subheader("üîç Global Filters")
        
        df = st.session_state.interface_df
        
        if 'source_system' in df.columns:
            sources = sorted(df['source_system'].dropna().unique())
            st.session_state.filter_source_systems = st.sidebar.multiselect(
                "Source System",
                options=sources,
                default=st.session_state.filter_source_systems
            )
        
        if 'target_system' in df.columns:
            targets = sorted(df['target_system'].dropna().unique())
            st.session_state.filter_target_systems = st.sidebar.multiselect(
                "Target System",
                options=targets,
                default=st.session_state.filter_target_systems
            )
        
        if 'type' in df.columns:
            types = sorted(df['type'].dropna().unique())
            st.session_state.filter_types = st.sidebar.multiselect(
                "Type",
                options=types,
                default=st.session_state.filter_types
            )
        
        if 'application' in df.columns:
            apps = sorted(df['application'].dropna().unique())
            st.session_state.filter_applications = st.sidebar.multiselect(
                "Application",
                options=apps,
                default=st.session_state.filter_applications
            )
        
        if get_active_filter_count() > 0:
            if st.sidebar.button("üóëÔ∏è Clear All Filters", use_container_width=True):
                clear_all_filters()
                st.rerun()
    
    # Navigation
    st.sidebar.markdown("---")
    st.sidebar.subheader("üìç Current Location")
    
    level = st.session_state.navigation_level
    
    if level == 'overview':
        st.sidebar.write("üè† 360¬∞ Overview")
    elif level == 'system_pair':
        st.sidebar.write(f"üîó {st.session_state.selected_source} ‚Üí {st.session_state.selected_target}")
    elif level == 'interface_detail':
        st.sidebar.write(f"üìã {st.session_state.selected_interface}")
    
    if level != 'overview':
        if st.sidebar.button("üè† Return to Overview", use_container_width=True):
            navigate_to_overview()
    
    # Main content
    if st.session_state.interface_df is None:
        st.markdown('<div class="premium-header"><h1 class="premium-title">üåê Welcome to Data Explorer 360¬∞</h1><p class="premium-subtitle">Enhanced Semantic Matching Edition</p></div>', unsafe_allow_html=True)
        
        st.info("üëà Upload your Interface Inventory and SQL Metadata in the sidebar")
        
        st.markdown("""
        ### ‚ú® Enhanced Matching Features
        
        This version uses **intelligent semantic matching** to find relationships between:
        
        **Interface Side:** Application, Integration, Description, Source System, Type, Target System
        
        **SQL Side:** System, File, QueryName, Tables
        
        **Matching Strategy:**
        - üéØ **Deterministic (70%)**: Exact matches on systems, names, types
        - ü§ñ **Semantic (30%)**: AI-powered similarity on descriptions and context
        
        **Result:** More accurate matches with confidence scores!
        """)
    else:
        if level == 'overview':
            render_overview()
        elif level == 'system_pair':
            render_system_pair()
        elif level == 'interface_detail':
            render_interface_detail()

if __name__ == "__main__":
    main()

